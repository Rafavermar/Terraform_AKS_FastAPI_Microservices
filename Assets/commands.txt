
===============================================================================================================================================================================================================================
A] Prerequisites
https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-windows?tabs=azure-cli
https://code.visualstudio.com/download
https://learn.microsoft.com/en-us/azure/developer/terraform/configure-vs-code-extension-for-terraform?tabs=azure-cli
https://developer.hashicorp.com/terraform/install
https://kubernetes.io/releases/download/ 

=================================================================================================================================================================================================================================
# Download azure cli
# Download terraform
# Edit environment variables and add the terraformn .exe directory to Path
# Download kubectl and edit environment variables and add it to PATH

# Create a folder in your proyect AKS Terraform > TerraformScriptsAKS

#In the IDE (VSCode or Pycharm)
- install Azure terraform plugin / Terraform plugin
- open the folder created before
- open the terminal in this folder
- be logged into Azure portal in the browser
- in the terminal of the IDE: az login
     - sso opens in the browser
     - finally you log in using azure cli

# Create terraform files:

  - provider.tf
  - ssh.tf
  - main.tf
  - variables.tf -- node count default 2
  - outputs.tf 


===============================================================================================================================================================================================================================
B] Implement the Terraform code

1) provider.tf
terraform {
  required_version = ">=1.0"

  required_providers {
    azapi = {
      source  = "azure/azapi"
      version = "~>1.5"
    }
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~>3.0"
    }
    random = {
      source  = "hashicorp/random"
      version = "~>3.0"
    }
    time = {
      source  = "hashicorp/time"
      version = "0.9.1"
    }
  }
}

provider "azurerm" {
  features {}
}


2) ssh.tf
resource "random_pet" "ssh_key_name" {
  prefix    = "ssh"
  separator = ""
}

resource "azapi_resource_action" "ssh_public_key_gen" {
  type        = "Microsoft.Compute/sshPublicKeys@2022-11-01"
  resource_id = azapi_resource.ssh_public_key.id
  action      = "generateKeyPair"
  method      = "POST"

  response_export_values = ["publicKey", "privateKey"]
}

resource "azapi_resource" "ssh_public_key" {
  type      = "Microsoft.Compute/sshPublicKeys@2022-11-01"
  name      = random_pet.ssh_key_name.id
  location  = azurerm_resource_group.rg.location
  parent_id = azurerm_resource_group.rg.id
}

output "key_data" {
  value = jsondecode(azapi_resource_action.ssh_public_key_gen.output).publicKey
}


3)main.tf
# Generate random resource group name
resource "random_pet" "rg_name" {
  prefix = var.resource_group_name_prefix
}

resource "azurerm_resource_group" "rg" {
  location = var.resource_group_location
  name     = random_pet.rg_name.id
}

resource "random_pet" "azurerm_kubernetes_cluster_name" {
  prefix = "cluster"
}

resource "random_pet" "azurerm_kubernetes_cluster_dns_prefix" {
  prefix = "dns"
}

resource "azurerm_kubernetes_cluster" "k8s" {
  location            = azurerm_resource_group.rg.location
  name                = random_pet.azurerm_kubernetes_cluster_name.id
  resource_group_name = azurerm_resource_group.rg.name
  dns_prefix          = random_pet.azurerm_kubernetes_cluster_dns_prefix.id

  identity {
    type = "SystemAssigned"
  }

  default_node_pool {
    name       = "agentpool"
    vm_size    = "Standard_D2_v2"
    node_count = var.node_count
  }
  linux_profile {
    admin_username = var.username

    ssh_key {
      key_data = jsondecode(azapi_resource_action.ssh_public_key_gen.output).publicKey
    }
  }
  network_profile {
    network_plugin    = "kubenet"
    load_balancer_sku = "standard"
  }
}



4)variables.tf
variable "resource_group_location" {
  type        = string
  default     = "eastus"
  description = "Location of the resource group."
}

variable "resource_group_name_prefix" {
  type        = string
  default     = "rg"
  description = "Prefix of the resource group name that's combined with a random ID so name is unique in your Azure subscription."
}

variable "node_count" {
  type        = number
  description = "The initial quantity of nodes for the node pool."
  default     = 2
}

variable "msi_id" {
  type        = string
  description = "The Managed Service Identity ID. Set this value if you're running this example using Managed Identity as the authentication method."
  default     = null
}

variable "username" {
  type        = string
  description = "The admin username for the new cluster."
  default     = "azureadmin"
}



5)outputs.tf
output "resource_group_name" {
  value = azurerm_resource_group.rg.name
}

output "kubernetes_cluster_name" {
  value = azurerm_kubernetes_cluster.k8s.name
}

output "client_certificate" {
  value     = azurerm_kubernetes_cluster.k8s.kube_config[0].client_certificate
  sensitive = true
}

output "client_key" {
  value     = azurerm_kubernetes_cluster.k8s.kube_config[0].client_key
  sensitive = true
}

output "cluster_ca_certificate" {
  value     = azurerm_kubernetes_cluster.k8s.kube_config[0].cluster_ca_certificate
  sensitive = true
}

output "cluster_password" {
  value     = azurerm_kubernetes_cluster.k8s.kube_config[0].password
  sensitive = true
}

output "cluster_username" {
  value     = azurerm_kubernetes_cluster.k8s.kube_config[0].username
  sensitive = true
}

output "host" {
  value     = azurerm_kubernetes_cluster.k8s.kube_config[0].host
  sensitive = true
}

output "kube_config" {
  value     = azurerm_kubernetes_cluster.k8s.kube_config_raw
  sensitive = true
}
===============================================================================================================================================================================================================================
C] Create Resources using Terraform 


terraform init -upgrade

terraform plan -out main.tfplan

terraform apply main.tfplan


===============================================================================================================================================================================================================================
D] Verify the results

# Go to azure portal and navigate to kubernetes services and observe the created kubernetes cluster.

Click on connect to be able to connect to the kubernetes cluster
 - in cloud shell copy the commnand: Set the cluster subscription
   " az account set --subscription 289293r23jlne..."

 - paste the command in VS/pycharm and run it

 - Copy the download cluster credentials command to Azure Kubernetes:
   " az aks get-credentials --resource-group rg-included........."
 - paste the command in VS/pycharm and run it


az account set --subscription 9e3c713b-0d42-41e5-81ca-d778fd001da7

az aks get-credentials --resource-group rg-eternal-sculpin --name cluster-relaxing-ant --overwrite-existing


kubectl get nodes

===============================================================================================================================================================================================================================
E] Deploy microservices FASTAPI apps for books and clients, related through an api gateway with personalized endpoints

docker login
docker build . -t jrvm/books-books-api:1.0.0
docker push jrvm/books-books-api:1.0.0

docker build . -t jrvm/books-clients-api:1.0.0
docker push jrvm/books-clients-api:1.0.0

docker build . -t jrvm/books-gateway-api:1.0.0
docker push jrvm/books-gateway-api:1.0.0

kubectl create -f books-data-pvc.yml
## An azurefile storageclass had to be added to allow the pod from different nodes to write to the volume claim

kubectl get pvc

kubectl create -f books-deployment.yml
kubectl get deployment
kubectl get po

-------------
## Scan a pod to confirm volume provisioning.
## we see that the csv is not there, so we have to copy it inside

kubectl exec -it books-api-689c477fc5-bqnf9 -- /bin/bash
> df -h
> ls /data
> exit

### from directory: \books\app\data
kubectl cp books.csv books-api-689c477fc5-bqnf9:/data/

## check that the file has been copied
kubectl exec -it books-api-689c477fc5-bqnf9 -- /bin/bash
> df -h
> ls /data
> exit

## portforward to localhost to check the operation of the service
kubectl port-forward books-api-689c477fc5-bqnf9 8080:8000

## now from the browser:
http://localhost:8080/docs

## We see that everything works and now we expose the service internally to the cluster and by default Kubernetes will create a service of type cluster ip, the service is only visible from the internal Kubernetes network.

kubectl expose deploy books-api --port 8000
kubectl get svc

kubectl create -f clients-data-pvc.yml
kubectl create -f clients-deployment.yml
kubectl get deployment
kubectl get po
kubectl exec -it clients-api-98b79bd4f-wbm4q -- /bin/bash
> df -h
> ls /data
> exit

### from directory: \clients\app\data
kubectl cp clients.csv clients-api-98b79bd4f-wbm4q:/data/

## check that the file has been copied
kubectl exec -it clients-api-98b79bd4f-wbm4q -- /bin/bash
> df -h
> ls /data
> exit

kubectl port-forward clients-api-98b79bd4f-wbm4q 8080:8000
kubectl expose deploy clients-api --port 8000
kubectl get deploy
kubectl get service
kubectl get pods

## A clearer directory is created to collect the file to be used in the configmap. Aiming at ..gateway\app\conf.d
New-Item -ItemType Directory -Path .\gateway-conf
Copy-Item -Path main.yml -Destination .\gateway-conf\


kubectl create cm books-gateway-conf --from-file gateway-conf/
kubectl get cm
kubectl get cm books-gateway-conf -o yaml

kubectl create -f gateway-deployment.yml
kubectl get deploy
kubectl get pod | grep gatewa

## in WINDOWS ##
kubectl get pod | ? { $_ -match "gateway" }

## Expose the gateway as a loadbalancer object. Since it is an AKS cluster, Azure will provide the external IP

kubectl expose deploy gateway-api --port 8000 --type LoadBalancer
kubectl get svc

## copy external IP and paste it in the browser like this:
172.210.92.195:8000/docs

--- Troubleshooting in case of returning empty lists for clients and books--
kubectl apply -f gateway-deployment.yml
kubectl edit deploy gateway-api
kubectl rollout restart deployment clients-api
kubectl rollout restart deployment books-api
kubectl rollout restart deployment gateway-api


===============================================================================================================================================================================================================================
F] Cleanup

kubectl get all

kubectl delete service/books-api 
kubectl delete service/clients-api
kubectl delete service/gateway-api

kubectl delete deployment books-api
kubectl delete deployment clients-api
kubectl delete deployment gateway-api

terraform plan -destroy -out main.destroy.tfplan 

terraform apply "main.destroy.tfplan"

az logout



